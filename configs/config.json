{
    "traj_file_path":"",
    "token":"",
    "model_name":"meta-llama/Llama-2-7b-chat-hf",
    "cache_dir":"../LLMTab/.cache",
    "batch_size":64,
    "lr":1e-4,
    "lora_alpha":32,
    "lora_dropout":0.02,
    "lora_r":16,
    "device":"cuda:0",
    "MAX_LENGTH":400,
    "num_epochs":20,
    "save_model_dir":"",
    "ft_model_path":"",
    "output_num":6300,
    "save_file_path":"",
    "save_file_path_hos":"",
    "is_departure":false,
    "temperature":1.2,
    "hos_file_path":"",
    "satisfied_indices_path":""
}